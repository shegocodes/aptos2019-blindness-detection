{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c84781d",
   "metadata": {},
   "source": [
    "# Uncomment to install all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy pandas matplotlib torch scikit-learn Pillow seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2a186",
   "metadata": {},
   "source": [
    "# Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, zipfile, fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU if available (defaults to CPU)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU for this demo...')\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print('Using CPU for this demo...')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec8f21",
   "metadata": {},
   "source": [
    "# Unzip imagefiles (training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cc93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile, fnmatch\n",
    "\n",
    "indir = 'train_images_resized_500'\n",
    "pattern = '*.zip'\n",
    "\n",
    "datadir = 'train_images_500' # where we will be storing all training images\n",
    "os.system(f'rm -r {datadir}')\n",
    "os.system(f'mkdir {datadir}')\n",
    "\n",
    "for root, dirs, files in os.walk(indir):\n",
    "    for filename in fnmatch.filter(files, pattern):\n",
    "        filepath = os.path.join(indir, filename)\n",
    "        \n",
    "        # Unzip files\n",
    "        print(f'Unzipping {filepath}')\n",
    "        os.system(f'unzip -q {filepath}')\n",
    "        \n",
    "        # Move all imagefiles under nested directory to main data directory\n",
    "        subdir = filepath.replace('.zip', '')\n",
    "        os.system(f'mv {subdir}/* {datadir}')\n",
    "        os.system(f'rm -r {subdir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022fbc9",
   "metadata": {},
   "source": [
    "# Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abspath(directory):\n",
    "    \"\"\"Function returns list of absolute paths under root directory\"\"\"\n",
    "    abspaths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            abspaths.append(os.path.join(root, file))\n",
    "    return abspaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8636dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filepath, dirpath):\n",
    "    \"\"\"Function takes in csv filepath and images directory\"\"\"\n",
    "    \n",
    "    # Read data into pandas dataframe\n",
    "    data = pd.read_csv(filepath)\n",
    "    #print(data.head())\n",
    "    \n",
    "    # Understanding data column types\n",
    "    print(f'* Get type of each data column: \\n {data.dtypes} \\n')\n",
    "\n",
    "    # Get all image files under directory\n",
    "    imagefiles = get_abspath(dirpath)\n",
    "    \n",
    "    # Create a new column for image paths\n",
    "    image_paths = []\n",
    "    for idx,row in data.iterrows():\n",
    "        id_code = row['id_code']\n",
    "        count = 0\n",
    "        for imagefile in imagefiles:\n",
    "            if id_code in imagefile:\n",
    "                count += 1\n",
    "                image_paths.append(imagefile)\n",
    "        if count == 0: image_paths.append('')\n",
    "    \n",
    "    # Update data with image paths column\n",
    "    data['image_path'] = image_paths\n",
    "    data = data[data['image_path']!=''] # in case there are missing image paths\n",
    "    print(f'* Updated data ({len(data)} rows)...')\n",
    "    print(f'{data.head()} \\n')\n",
    "\n",
    "    # Use grouby and count to see if dataset is balanced\n",
    "    try:\n",
    "        count = data.groupby(['diagnosis'])['diagnosis'].count()\n",
    "        print(f'* Get count of each class label: \\n {count}')\n",
    "    except: pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d27809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (which will be split for training and validation)\n",
    "trainfile = 'train.csv'\n",
    "data = preprocess(trainfile, datadir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff7f5c22",
   "metadata": {},
   "source": [
    "My understanding of this dataset is that it is imbalanced due to the fact that it is disproportional in class samples. For instance, label class 0 (No DR) is significantly greater than label class 4 (Proliferative DR).\n",
    "\n",
    "Forseeable issues due to class imbalance:\n",
    "* Bias towards majority class which can result in poor performance on minority class.\n",
    "\n",
    "Solution:\n",
    "* Undersample majority class by using only 150 samples from each class to balanced out this dataset.\n",
    "* Use precision and recall to evaluate your model instead of accuracy alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0a98d",
   "metadata": {},
   "source": [
    "# Display training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = sample(get_abspath(datadir), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "for image in sample_images:\n",
    "    plt.figure()\n",
    "    img = mpimg.imread(image)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.title(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8749b2",
   "metadata": {},
   "source": [
    "# Split dataset (training + validation + testing)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4da297b5",
   "metadata": {},
   "source": [
    "Difference between training, validation, and testing datasets:\n",
    "\n",
    "* Training data: data used for teaching the model using labeled data\n",
    "* Validation data: data used for fine-tuning and model selection during development\n",
    "* Testing data: data used for evaluating model's performance on unseen data\n",
    "\n",
    "Properly splitting the data into training, testing, and validation datasets is crucial for building robust and reliable machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, validation_set = train_test_split(data, test_size=0.15)\n",
    "print(f'Training: {len(train_set)} Validation: {len(validation_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data,reshape=True,height=128,width=128,autoencoder=False):\n",
    "        \n",
    "        self.no_class = data[data['diagnosis']==0]['image_path'].tolist()\n",
    "        self.mild_class = data[data['diagnosis']==1]['image_path'].tolist()\n",
    "        self.moderate_class = data[data['diagnosis']==2]['image_path'].tolist()\n",
    "        self.severe_class = data[data['diagnosis']==3]['image_path'].tolist()\n",
    "        self.proliferative_class = data[data['diagnosis']==4]['image_path'].tolist()\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.reshape = reshape\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "        labels = [0 for i in range(len(self.no_class))]\n",
    "        labels += [1 for i in range(len(self.mild_class))]\n",
    "        labels += [2 for i in range(len(self.moderate_class))]\n",
    "        labels += [3 for i in range(len(self.severe_class))]\n",
    "        labels += [4 for i in range(len(self.proliferative_class))]\n",
    "\n",
    "        links = self.no_class + self.mild_class + self.moderate_class + self.severe_class + self.proliferative_class\n",
    "\n",
    "        self.dataframe = pd.DataFrame({\"image\":links, \"labels\":labels})\n",
    "        self.dataframe.reset_index(inplace = True ,drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        no_size = len(self.no_class)\n",
    "        mild_size = len(self.mild_class)\n",
    "        moderate_size = len(self.moderate_class)\n",
    "        severe_size = len(self.severe_class)\n",
    "        proliferative_size = len(self.proliferative_class)\n",
    "        \n",
    "        return no_size + mild_size + moderate_size + severe_size + proliferative_size\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        image_list = self.dataframe[\"image\"][idx]\n",
    "        label_list = self.dataframe[\"labels\"][idx]\n",
    "\n",
    "        if type(image_list) == str: \n",
    "            image_list = [image_list]\n",
    "            \n",
    "        if not isinstance(label_list,np.int64):\n",
    "            label_list = label_list.values\n",
    "            \n",
    "        image_array = []\n",
    "        \n",
    "        for image in image_list:\n",
    "            image = Image.open(image).convert(\"L\")\n",
    "            \n",
    "            if self.reshape:\n",
    "                image = image.resize((self.height,self.width))\n",
    "                \n",
    "            array = np.asarray(image)\n",
    "            \n",
    "            array = array.reshape(1,self.height,self.width)\n",
    "            \n",
    "            image_array.append(array)\n",
    "            \n",
    "        return [torch.tensor(np.array(image_array),device=device),torch.tensor(label_list,device=device)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "\n",
    "        # Use grouby and count to see if dataset is balanced\n",
    "        count = self.dataframe.groupby(['labels'])['labels'].count()\n",
    "        print(f'* Get count of each class label: \\n{count}\\n')\n",
    "\n",
    "        return str(self.dataframe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438f45a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = MyDataset(train_set)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = MyDataset(validation_set)\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2417a9",
   "metadata": {},
   "source": [
    "# Custom CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):      \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1,256,kernel_size=3), # 126*126*256\n",
    "            nn.MaxPool2d(2,2), # 63*63*256\n",
    "            nn.Conv2d(256,32,kernel_size=2) # 63-2+1 = 62*62*32\n",
    "        )\n",
    "        \n",
    "        # n-f+2p/s +1 \n",
    "        \n",
    "        self.linear1 = nn.Linear(62,128)\n",
    "        self.linear2 = nn.Linear(128,64)\n",
    "        self.flat = nn.Flatten(1)\n",
    "        self.linear3 = nn.Linear(126976,5) # 5 b/c there are 5 classes/labels\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for n in range(len(train_set)//batch_size):\n",
    "        data , target = train_set[n*batch_size : (n+1)*batch_size]\n",
    "        \n",
    "        ypred = model.forward(data.float())\n",
    "        loss = loss_fn(ypred,target)\n",
    "        \n",
    "        total_loss+=loss\n",
    "        \n",
    "        optimizer.zero_grad() # clear the gradients\n",
    "        loss.backward() # calculate the gradient\n",
    "        optimizer.step() # Wn = Wo - lr * gradient\n",
    "        \n",
    "    loss_list.append(total_loss/batch_size)\n",
    "\n",
    "    print(\"Epochs {}  Training Loss {:.2f}\".format(epoch+1,total_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d884956",
   "metadata": {},
   "source": [
    "# Save out trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'ellie-trained-model-cpu-500.pth'\n",
    "torch.save(model.state_dict(), modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e74d4c",
   "metadata": {},
   "source": [
    "# Display convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac47273",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(list(range(epochs)),[x.tolist() for x in loss_list])\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da60c5",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04f89036",
   "metadata": {},
   "source": [
    "Formulas:\n",
    "\n",
    "• accuracy = total number of correct predictions / total number of predictions\n",
    "• precision = true positives / (true positives + false positives)\n",
    "• recall = true positives / (true positives + false negatives)\n",
    "\n",
    "Notes:\n",
    "\n",
    "• Precision and recall are useful in cases where classes aren't evenly distributed.\n",
    "• Precision focuses on the accuracy of positive predictions, while recall focuses on the ability of the model to capture all positive instances.\n",
    "• A high precision indicates that the model is making accurate positive predictions, while a high recall indicates that the model is capturing a large proportion of the actual positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate(model, validation_set, plot='no'):\n",
    "    \"\"\"Function evaluates model by calculating accuracy\"\"\"\n",
    "    \n",
    "    # Map labels to reference class\n",
    "    mapping = {0:\"No DR\", 1:\"Mild\", 2:\"Moderate\", 3:\"Severe\", 4:\"Proliferative DR\"}\n",
    "\n",
    "    # Ground truths vs. model predictions\n",
    "    total = len(validation_set)\n",
    "    \n",
    "    actuals, predictions = [], []\n",
    "    for i in range(total):\n",
    "        print(f'Predicting {i+1}/{total}...')\n",
    "        \n",
    "        data, target = validation_set[i]\n",
    "        \n",
    "        pred = torch.argmax(model.forward(data.float()), dim=1)\n",
    "        \n",
    "        actual = mapping[target.cpu().detach().item()]\n",
    "        prediction = mapping[pred.cpu().detach().item()]\n",
    "        \n",
    "        actuals.append(actual)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "        # Plot results...\n",
    "        if plot == 'yes':\n",
    "            plt.figure()\n",
    "            plt.imshow(data[0][0].cpu())\n",
    "            plt.title(f\"Actual : {actual} | Prediction : {prediction}\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Build confusion matrix\n",
    "    plt.figure(figsize=(10,6))\n",
    "    fx=sns.heatmap(confusion_matrix(actuals,predictions), annot=True, fmt=\".2f\", cmap=\"GnBu\")\n",
    "    fx.set_title('Confusion Matrix \\n');\n",
    "    fx.set_xlabel('\\n Predicted Values\\n')\n",
    "    fx.set_ylabel('Actual Values\\n');\n",
    "    fx.xaxis.set_ticklabels(mapping.values())\n",
    "    fx.yaxis.set_ticklabels(mapping.values())\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(classification_report(actuals, predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede6a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(model, validation_set, plot='no')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8d8e0",
   "metadata": {},
   "source": [
    "# (Optional) Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd53923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_training(model, train_set, loss_list, epochs=5, batch_size=32):\n",
    "    \"\"\"Function continues training your model\"\"\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "    \n",
    "        for n in range(len(train_set)//batch_size):\n",
    "            data , target = train_set[n*batch_size : (n+1)*batch_size]\n",
    "        \n",
    "            ypred = model.forward(data.float())\n",
    "            loss = loss_fn(ypred,target)\n",
    "        \n",
    "            total_loss+=loss\n",
    "        \n",
    "            optimizer.zero_grad() # clear the gradients\n",
    "            loss.backward() # calculate the gradient\n",
    "            optimizer.step() # Wn = Wo - lr * gradient\n",
    "        \n",
    "        loss_list.append(total_loss/batch_size)\n",
    "\n",
    "        print(\"Epochs {}  Training Loss {:.2f}\".format(epoch+1,total_loss/n))\n",
    "        \n",
    "    return model, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass\n",
    "updated_model, updated_loss_list = continue_training(model, train_set, loss_list, epochs=5, batch_size=32)\n",
    "\n",
    "# Save out updated model\n",
    "modelname = 'ellie-trained-model-cpu-500-updated.pth'\n",
    "torch.save(updated_model.state_dict(), modelname)\n",
    "\n",
    "# Evaluate with updated model\n",
    "evaluate(updated_model, validation_set, plot='no')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8d733",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "362f0b44",
   "metadata": {},
   "source": [
    "Accuracy went up 3% with continued training from 46% to 49%. \n",
    "\n",
    "* no_dr: high precision, high recall\n",
    "* mild_dr: low precision, low recall\n",
    "* moderate_dr: low precision, low recall\n",
    "* severe_dr: low precision, low recall\n",
    "* proliferative_dr: low precision, high recall\n",
    "\n",
    "What does it mean when both precision and recall are low**?\n",
    "* Model is not performing well in identifying either the positive or negative class correctly. \n",
    "\n",
    "What does it mean when precision is low but recall is high?**\n",
    "* Model is correctly identifying many instances of the positive class (true positives) but is also incorrectly identifying many instances of the negative class as belonging to the positive class (false positives).\n",
    "\n",
    "What does it mean when both precision and recall are high?**\n",
    "* Model is performing well in identifying both the positive and negative class correctly. Model is reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
